# the teams or use cases accessing Azure OpenAI (aka. "clients")
# notes: - each client must have a unique key. PowerProxy identifies clients by the given key.
#        - one and only one client can use "uses_entra_id_auth: true" instead of a key. this client is used for all
#          requests where an Entra ID/Azure AD authentication shall be used instead of an API key authentication.
clients:
  - name: Team 1
    description: An example team named 'Team 1'.
    key: 28ef59ae30342978f81c4ad96ce47ab
    max_tokens_per_minute_in_k:
      gpt-35-turbo: 20
      gpt-4-turbo: 5
    deployments_allowed: gpt-35-turbo, gpt-4-turbo
  - name: Team 2
    description: An example team named 'Team 2'.
    key: 1113456789abcdef0123456789abcde
    max_tokens_per_minute_in_k: 30
    deployments_allowed: gpt-35-turbo, gpt-4-turbo
  - name: Test Script Client 1
    description: Used by the included test scripts.
    key: 04ae14bc78184621d37f1ce57a52eb7
    max_tokens_per_minute_in_k: 5
    deployments_allowed: gpt-35-turbo, gpt-4-turbo
  - name: Test Script Client 2
    description: Used by the included test scripts.
    key: 72bd81ef32763530b29e3da63d46ad6
    max_tokens_per_minute_in_k: 5
    deployments_allowed: gpt-35-turbo, gpt-4-turbo
  - name: Entra ID Auth Client
    description: Client used whenever a client authenticates via Entra ID / Azure AD.
    uses_entra_id_auth: true
    max_tokens_per_minute_in_k: 10
    deployments_allowed: gpt-35-turbo, gpt-4-turbo

# defines the plugins enabled for the proxy
plugins:
  - name: AllowDeployments
  - name: LimitUsage
    # remove the redis field if no redis synchronization is desired
    # note: do that only in case of a single PowerProxy worker where no synchronization is needed
    redis:
      redis_host: <will be set by deployment script>
      redis_password: <will be set by deployment script>
  - name: LogUsageToConsole
  - name: LogUsageToCsvFile
  - name: LogUsageToLogAnalytics
    log_ingestion_endpoint: <will be set by deployment script>
    data_collection_rule_id: <will be set by deployment script>
    # by default, we use the managed identity in the resource group to auth against Log Analytics.
    # if the managed identity cannot be used, as an alternative, credentials of a service principal
    # can be specified here. make sure that the managed identity/service principle have the
    # "Monitoring Metrics Publisher" role assigned to the Data Collection Rule (it might take up
    # to 30 minutes to become effective after configuration).
    #credential_tenant_id: ___
    #credential_client_id: ___
    #credential_client_secret: ___

# Azure OpenAI
aoai:
  endpoints:
    - name: Some Endpoint
      url: https://___.openai.azure.com/
      # not required when Azure OpenAI's Azure AD/Entra ID authentication is used
      key: ___
      # fraction of non-streaming requests handled
      # 0   = endpoint will handle no non-streaming request
      # 0.7 = endpoint will handle 70% of the non-streaming requests it gets
      # 1   = endpoint will handle all non-streaming request it gets
      non_streaming_fraction: 1

    - name: Another Endpoint
      url: https://___.openai.azure.com/
      # not required when Azure OpenAI's Azure AD/Entra ID authentication is used
      key: ___
      # fraction of non-streaming requests handled
      # 0   = endpoint will handle no non-streaming request
      # 0.7 = endpoint will handle 70% of the non-streaming requests it gets
      # 1   = endpoint will handle all non-streaming request it gets
      non_streaming_fraction: 1

      # endpoints can also have "virtual deployments" (optional). when a virtual deployment is defined, requests
      # requesting specific deployments are rewritten such that a smart load balancing across the listed "stand-ins"
      # = real deployments at the endpoint happens. similar to the non_streaming_fraction at the endpoint level, we
      # can also set non_streaming_fractions for stand-ins.
      virtual_deployments:
        - name: gpt-35-turbo
          standins:
            - name: gpt-35-turbo-ptu
              non_streaming_fraction: 0.2
            - name: gpt-35-turbo-paygo
        - name: gpt-4-turbo
          standins:
            - name: gpt-4-turbo-ptu
              non_streaming_fraction: 0.2
            - name: gpt-4-turbo-paygo
  # # alternatively, specify a mock response to be used instead of the real response from
  # # Azure OpenAI
  # # note: use this for testing PowerProxy's scalability
  # mock_response:
  #   ms_to_wait_before_return: 1000
  #   json: {
  #     "id": "chatcmpl-87lITNUXLFIBHyDu3jFTtgOibcAxz",
  #     "object": "chat.completion",
  #     "created": 1696860797,
  #     "model": "gpt-35-turbo",
  #     "choices": [
  #       {
  #         "index": 0,
  #         "finish_reason": "stop",
  #         "message": {
  #           "role": "assistant",
  #           "content": "I'm a mock response and not a real answer from Azure OpenAI."
  #         }
  #       }
  #     ],
  #     "usage": {
  #       "completion_tokens": 16,
  #       "prompt_tokens": 61,
  #       "total_tokens": 77
  #     }
  #   }

# id of the Azure subscription where the proxy shall be deployed to
azure_subscription_id: ___

# region to which the proxy shall be deployed to Azure
# example: westeurope
region: ___

# resource group to which the proxy shall be deployed to Azure
# example: PowerProxy-AOAI
resource_group: ___

# unique prefix to prepend to certain resource names to avoid naming conflicts.
# example: abcde
unique_prefix: ___

# id of the user-assigned managed identity
# note: PowerProxy will assume a system-assigned managed identity if not specified
user_assigned_managed_identity_client_id: <will be set by deployment script>
